---
title: "Excercise 1"
output: md_document
---
DATA VISUALIZATION

```{r}
set.seed(1234)
library(ggplot2)
library(tidyverse)

data <- read.csv("~/Desktop/SDS323-master/data/ABIA.csv")

# clean the data
data$Year <- NULL
data$Month <- as.factor(data$Month)
data$DayofMonth <- as.factor(data$DayofMonth)
data$DayOfWeek <- as.factor(data$DayOfWeek)
data$Cancelled <- as.factor(data$Cancelled)
data$Diverted <- as.factor(data$Diverted)
data$Delay <- data$DepDelay + data$ArrDelay
summary(data)

# change the levels to make the data more readable
levels <- levels(data$DayOfWeek)
levels <- c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")
levels(data$DayOfWeek) <- levels

levels <- levels(data$Month)
levels <- c("January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December")
levels(data$Month) <- levels
```

```{r}
set.seed(1234)

# group data by DayOfWeek
delay_summ <- data %>% group_by(DayOfWeek) %>% summarize(sum_delay.mean = mean(DepDelay + ArrDelay, na.rm = TRUE))

ggplot(data = delay_summ, aes(x = reorder(DayOfWeek, sum_delay.mean), y = sum_delay.mean, fill=DayOfWeek)) + geom_bar(stat="identity") + labs(x = "Day", y = "Mean Delay", title = "Flight Delay by Weekday") + coord_flip()

# group data by Month
month <- data %>% group_by(Month) %>% summarize(delay = mean(DepDelay + ArrDelay, na.rm = TRUE))

ggplot(data = month, aes(x = reorder(Month, delay), y = delay, fill=Month)) + geom_bar(stat = "identity") + labs(x = "Month", y = "Mean Delay", title = "Flight Delay by Month") + coord_flip()

saturday <- subset(data, data$DayOfWeek=="Saturday")
wednesday <- subset(data, data$DayOfWeek=="Wednesday")

# bootstrap for mean delay on saturday and wednesday
delay.saturday <- c()
delay.wednesday <- c()
for(i in 1:1000) {
  x <- saturday[sample(nrow(saturday), replace = TRUE),]
  delay.saturday[i] <- mean(x$DepDelay + x$ArrDelay, na.rm = TRUE)
  y <- wednesday[sample(nrow(saturday), replace = TRUE),]
  delay.wednesday[i] <- mean(y$DepDelay + y$ArrDelay, na.rm = TRUE)
}

hist(delay.saturday)
qqnorm(delay.saturday)
qqline(delay.saturday)

hist(delay.wednesday)
qqnorm(delay.wednesday)
qqline(delay.wednesday)
# the bootstrap distributions are approximately normal

# 95% confidence intervals
quantile(delay.saturday, c(0.025, 0.975))
quantile(delay.wednesday, c(0.025, 0.975))

best <- subset(data, data$Month=="September" & (data$DayOfWeek=="Saturday" | data$DayOfWeek=="Wednesday"))

# bootstrap for "best" travel days
means <- replicate(1000, mean(sample(best$Delay, nrow(best), replace = TRUE), na.rm = TRUE))

hist(means)
qqnorm(means)
qqline(means)
# the bootstrap distribution is approximately normal

# 95% confidence interval
quantile(means, c(0.025, 0.975))
```

I think the plots showcase my point without explanation, but I have also provided a more detailed explanation. To better analyze the data I defined delay as the sum of departure and arrival delay. When flying on a Saturday, we can say with 95% confidence that the average delay (a sum of departure and arrival delay) will be between 11.04 and 13.38 minutes. We know this because the bootstrap distribution is approximately normal. For wednesday, with 95% confidence the average delay will be between 11.44 and 13.61 minues. When analyzing delay by month, 3 distinct groups appear: September, October, and November easily have the shortest average delays; January, April, May, July, August, and February; and June, March, and December. June is the begining of summer and the end of the school year, March has spring break for UT Austin and other nearby universities (when many students will be flying in and out of AUS on the same day), and December is the worst travel month of the year because of Christmas and winter break.
The best days to fly out of AUS are Wednesday and Saturday, and the best months are September, October, and November. If we had all of the freedom in the world to plan our flight we would choose to fly in and out of AUS on Wednesday and Saturday of September. Flying out on one of these ideal days, with 95% confidence we can expect our delay to be between -2.61 and 0.34 minutes. Meaning we will likely have no delays, and even more so our flights will be shorter than advertised!














REGRESSION PRACTICE


Import the raw data
```{r}
data <- read.csv("~/Desktop/SDS323-master/data/creatinine.csv")
summary(data)
```

Plot the data
```{r}
library(ggplot2)

linear <- lm(data = data, creatclear ~ age)
ggplot(data=data, aes(x=data$age, y = data$creatclear)) + geom_point() + geom_smooth(method = 'lm', se = FALSE)

summary(linear)
```

A linear regression seems reasonable for this data set given the shape and spread of the data. Here we will predict the creatine clearance rate for a 55 year old.
```{r}
age <- c(40, 55, 60)
df <- data.frame(age)
pred <- predict(linear, df)
print(pred)
```

We should expect a creatine clearance rate of 113.7 (the raw data is rounded to the 10th decimal place). Creatine clear rate changes by -0.6 ml/minute per year. The 40 year old with a creatine clearance rate of 135 is healthier than a 60 year olf with 112, because the average creatine clearance rates are 123.0 and 110.6 for 40 and 60 year olds respectively. Therefore the 40 year old is much healthier.



GREEN BUILDINGS

I disagree with the way the "excel guru" analyzed the data. Scrapping buildings with < 10% occupancy is a poor decision because green certification could have a large role in occupancy. For example, it's possible that because green buildings are more expensive to construct that they then must  charge more in rent, leading to lower occupancy. This may not be a problem for small occupancy loses, but buildings with < 10% are likely losing a substantial abount of money, and are therefore important for our analysis. Subtracting median rent for green and non-green buildings is too simple and is likely ignoring compounding variables. For example, because green buildings tend to cost more to build, it is likely they are more common in wealthier areas where they can charge more rent to compensate for the building costs. Because this may not be true in other areas, it is important to compare green and non-green building rent within their clusters.
```{r}
library(tidyverse)
library(knitr)

green <- read.csv("~/Documents/R/SDS 323/SDS323-master/data/greenbuildings.csv")

# create new variables which represent the setting better
green$RelativeRent <- green$Rent - green$cluster_rent
green$TotalRent <- green$Rent*green$size*green$leasing_rate
green$RelativeTotalRent <- green$RelativeRent*green$size*green$leasing_rate

# make green rating a factor
green$green_rating <- factor(green$green_rating)
levels <- levels(green$green_rating)
levels <- c("No", "Yes")
levels(green$green_rating) <- levels

# group data by green rating
d1 <- green %>% group_by(green_rating) %>% summarize(mean = mean(RelativeRent), sd = sd(RelativeRent))

# plot mean RelativeRent vs green_rating 
ggplot(data = d1) + geom_bar(mapping = aes(x = green_rating, y = mean, fill = green_rating), stat = "identity") + labs(x = "Green Certification", y = "Mean Relative Rent (per square foot)", title = "Relative Rent for Green and Non-Green Buildings", fill = "Green Certification")

# the plot seems to indicate that green buildings are more profitable, but these distributions have large standard deviations 10 and 6.9 respectively (non-green, green)

# bootstrap
g <- subset(green, green$green_rating=="Yes")
g.mean <- c()
r <- subset(green, green$green_rating=="No")
r.mean <- c()
for(i in 1:1000) {
  x <- g[sample(nrow(g), replace = TRUE),]
  g.mean[i] <- mean(x$RelativeTotalRent)
  y <- r[sample(nrow(r), replace = TRUE),]
  r.mean[i] <- mean(y$RelativeTotalRent)
}

hist(g.mean)
qqnorm(g.mean)
qqline(g.mean)
quantile(g.mean, c(0.025, 0.975))

hist(r.mean)
qqnorm(r.mean)
qqline(r.mean)
quantile(r.mean, c(0.025, 0.975))
# both of the bootstrap distributions are approximately normal

sd(g.mean)
sd(r.mean)
# the two distributions have different sd => populations have different variances

# compute two sided t-test for the bootstrap distributions with different variances
t.test(x = g.mean, y = r.mean, var.equal = FALSE)

```
With 95% confidence we can say there is a difference in means between RelativeTotalRent for green and non-green buildings, with the means being 114,720,650 and 84,097,855 respectively. The 95% CI for the difference in means is (29,633,275, 31,612,314).

In order to relate this result to our problem, I will investigate if this relation follows for buildings more like the one we want to build. Specifically, buildings that have between 10 and 20 stories.
```{r}
# bootstrap with different subset of buildings
tall <- subset(green, subset = green$stories >= 10 & green$stories < 20)
g <- subset(tall, tall$green_rating=="Yes")
r <- subset(tall, tall$green_rating=="No")
g.mean <- c()
r.mean <- c()
for(i in 1:1000) {
  x <- g[sample(nrow(g), replace = TRUE),]
  y <- r[sample(nrow(r), replace = TRUE),]
  g.mean[i] <- mean(x$RelativeTotalRent)
  r.mean[i] <- mean(y$RelativeTotalRent)
}

hist(g.mean)
hist(r.mean)

qqnorm(g.mean)
qqline(g.mean)
qqnorm(r.mean)
qqline(r.mean)
# bootstrap distributions are approximately normal

sd(g.mean)
sd(r.mean)
# the standard deviations are substantially different => different variances

# expected difference in means
mean(g.mean - r.mean)
# two sided t-test for difference in means with different variances
t.test(x = g.mean, y = r.mean, var.equal = FALSE, conf.level = 0.9999)

```
This is a large investment for any company so I decided to use a 99.99%  confidence level for our t-test for difference in means. We will see that we would come to the same conclusions with practically any confidence level. With 99.99% confidence we can expect the RelativeTotalRent for buildings with 10 to 20 stories, to be within the interval (42,379,397, 45,086,090). Where the expected difference is centered about 43,732,744 per year. According to the assignment, constructing a green building is expected to cost an additional 5 million dollars. So with an expected 43 million dollars in revenue per year, we would expect to recoup the green ceritification costs within the first year and begin increasing our profit. Because of this I would say that investing in constructing a green building is a wise decision. If I knew which clusters were near the I-35/East Cesar Chavez clusters (where our building is being built), I would redo the previous analysis with similar clusters. However we know that the difference in means is statistically significant (p-value < 2.2e-16).



 


MILK PRICES


```{r}
library(ggplot2)

milk <- read.csv("~/Documents/R/SDS 323/SDS323-master/data/milk.csv")

ggplot(data = milk, aes(x = price, y = sales)) + geom_point()
```

The data appears to be quadratic or an inverse power (i.e y = x^-a)

```{r}
model1 <- glm(data = milk, sales ~ price)
model2 <- glm(data = milk, sales ~ poly(x = price, degree = 2))
model3 <- glm(data = milk, sales ~ poly(x = price, degree = 3))
model4 <- glm(data = milk, sales ~ poly(x = price, degree = 4))
f1 <- function(x) 133.4321 - 60.0686*x + 7.2914*x^2
f2 <- function(x) 236.6667 - 171.5551*x + 44.3760*x^2 - 3.8469*x^3
f3 <- function(x) exp(4.7206) * x^(-1.6186)

# plotting log(sales) vs log(price) indicates a roughly linear relationship between the two, this is motivated by economic theory
ggplot(data = milk, aes(x = log(price), y = log(sales))) + geom_point()
model5 <- glm(data = milk, log(sales) ~ log(price))

# LOOCV
library(boot)
cv.glm(data = milk, model1)$delta[1]
cv.glm(data = milk, model2)$delta[1]
cv.glm(data = milk, model3)$delta[1]
cv.glm(data = milk, model4)$delta[1]
cv.glm(data = milk, model5)$delta[1]

# of the polynomial models, model3 has the smallest MSE, just barely smaller than model4. However with model4 I am afraid of overfitting the data. Model5 performs much better than all of the polynomial models, as it should.

ggplot(data = milk, aes(x = price, y = sales)) + geom_point() + stat_function(fun = f2, color = "blue") + stat_function(fun = f1, color = "red") + stat_function(fun = f3, color = "green")
```
We can see that the red line tails upwards as price increases which is most likely a failure of the model rather than representative of the actual data. Hence why the blue line (model 3) is the more accurate and appropriate polynomial fit. However the green line appears to fit the data even better, and the LOOCV supports this, so we will use the power model to fit our data.

```{r}
# N - net profit
# c - whole sale cost per carton (given)
# P - per unit price
# Q - units sold

# N = (P-c)*Q
# Q = exp(4.7206) * P^(-1.6186)
# N = (P-c)*(exp(4.7206) * P^(-1.6186))
# N'(P) = c*181.664*x^(-2.6186)-69.4289*x^(-1.6186)

library(rootSolve)

# c can be set to any number >= 0
c <- 1
# interval to test over, may need to be expanded for larger values of c
interval <- c(0, 10)

# our functions for net profit
n <- function(P) (P-c)*(exp(4.7206)*P^(-1.6186))
n.prime <- function(P) c*181.664*P^(-2.6186)-69.4289*P^(-1.6186)

# there will only ever be one critical point with this function
root <- uniroot.all(n.prime, interval)

# print x = max and f(max)
print(root)
n(root)

# plot net profit vs price given c
ggplot(data = data.frame(x=0), mapping = aes(x = x)) + 
  scale_x_continuous(limits = interval) +
  ylim(0, NA) +
  stat_function(fun = n) +
  xlab("Price") + 
  ylab("Net Profit")
```
Given c >= P, this graph plots net profit vs price of milk. For c = 1, we can see that maximum profit occurs around P = 2.5 and net profit seems to be slightly less thatn 40. Solving the function directly corroborates this as the maximum occurs at P = 2.62 and f(P) = 38.25. So for a wholesale cost of 1 dollar we would maximize our profit by charging 2.62 dollars per unit of milk. By varying c we can easily find the maximum net profit for any c.



